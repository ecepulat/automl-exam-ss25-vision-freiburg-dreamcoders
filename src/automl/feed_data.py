import os
from PIL import Image
from torch.utils.data import Dataset
from utils import get_default_transforms, get_augmented_transforms
from torchvision.transforms import Resize, ToTensor, Compose


class BalancedDataset(Dataset):
    def __init__(self, dataframe, images_path, metadata):
        """
        Creates a dataset that duplicates samples from undersampled classes
        and applies on-the-fly augmentation only to those duplicated samples.

        Args:
            dataframe (pd.DataFrame): ['image_file_name', 'label']
            images_path (str): Folder where image files are located
            metadata (dict): Contains image size, imbalance flag, and undersampling info
        """
        self.df = dataframe
        self.images_path = images_path
        self.metadata = metadata

        # Undersampled class info (generated by get_undersampled_classes())
        self.undersampled_classes = metadata.get("undersampled_classes", {})

        # Read image resolution from metadata (e.g., (128, 128))
        self.image_resolution = metadata["image_resolution"]

        # Flag for global imbalance (optional, for analysis/debug only)
        self.dataset_is_imbalanced = metadata.get("is_imbalanced", False)

        # Set to log which augmentation ops were applied (shared with transforms)
        self.applied_ops_logger = set()

        # Final list of data entries (img_path, label, duplicated_flag)
        self.data = []

        # Track how many duplicates we created per class (for debugging)
        self.duplication_log = {}

        #if self.undersampled_classes:
            #print(f"[DEBUG] Detected undersampled classes: {list(self.undersampled_classes.keys())}")

        # Main loop to prepare data
        for _, row in self.df.iterrows():
            label = str(row['label'])  # convert label to string for dictionary key
            img_path = os.path.join(images_path, row['image_file_name'])

            # Always add the original image (not duplicated)
            self.data.append((img_path, label, False))

            # If this label is undersampled, create duplicates for augmentation
            if label in self.undersampled_classes:
                multiplier = self.undersampled_classes[label]['augmentation_multiplier']
                repeat_factor = max(int(multiplier - 1), 0)

                if repeat_factor > 0:

                    self.data.extend([(img_path, label, True)] * repeat_factor)
                    self.duplication_log[label] = self.duplication_log.get(label, 0) + repeat_factor

        print(f"[DEBUG] Final duplication log: {self.duplication_log}")

        # Define transforms
        self.default_transform = get_default_transforms(self.metadata)  # e.g., Resize + Normalize
        self.trivial_transform = get_augmented_transforms(self.metadata)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        """
        Loads an image and applies transformations.
        Applies random augmentation only to duplicated samples.
        """
        img_path, label, duplicated = self.data[idx]
        image = Image.open(img_path).convert("RGB")

        if duplicated:
            image = self.trivial_transform(image)
        else:
            image = self.default_transform(image)

        return image, int(label)
